{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "notebook_dir = os.path.abspath(os.path.dirname(\"__file__\"))\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, \"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9831e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Funk dataset and clean\n",
    "funk = pd.read_excel(\n",
    "    os.path.join(project_root, \"data/benchmarks/1-s2.0-S0092867422013599-mmc4.xlsx\"),\n",
    "    skiprows=1,\n",
    ")\n",
    "# Select only the rows where cluster is in 149, 121, 21, 167, 197, 37\n",
    "funk = funk[funk[\"Interphase cluster\"].isin([149, 121, 21, 167, 197, 37])].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "# Rename \"Gene symbol\" to \"gene_symbol\" and \"Interphase cluster\" to \"cluster\"\n",
    "funk.rename(\n",
    "    columns={\n",
    "        \"Gene symbol\": \"gene_symbol\",\n",
    "        \"Interphase cluster\": \"cluster\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# Drop all other columns\n",
    "funk = funk[[\"cluster\", \"gene_symbol\"]]\n",
    "# Sort by cluster\n",
    "funk.sort_values(by=[\"cluster\"], inplace=True)\n",
    "# Display the Funk dataset\n",
    "display(funk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aea9e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Wainberg dataset and clean\n",
    "wainberg = pd.read_excel(\n",
    "    os.path.join(project_root, \"data/benchmarks/Supplementary_Data_2.xlsx\"),\n",
    "    sheet_name=\"Co-essential Modules\",\n",
    "    skiprows=(0, 1),\n",
    ")\n",
    "# Select Module # 2067 and 2213\n",
    "wainberg = wainberg[wainberg[\"Module #\"].isin([2067, 2213])].reset_index(drop=True)\n",
    "# Create a new dataframe to store the reshaped data\n",
    "reshaped_data = []\n",
    "# Process each row in the original dataframe\n",
    "for _, row in wainberg.iterrows():\n",
    "    module_num = row[\"Module #\"]\n",
    "\n",
    "    # Process 'Genes' column\n",
    "    if pd.notna(row[\"Genes\"]):\n",
    "        reshaped_data.append({\"cluster\": module_num, \"gene_symbol\": row[\"Genes\"]})\n",
    "\n",
    "    # Process all the 'Unnamed: X' columns that contain gene symbols\n",
    "    for col in wainberg.columns:\n",
    "        if col.startswith(\"Unnamed:\") and pd.notna(row[col]):\n",
    "            # Skip empty cells or non-gene values\n",
    "            if isinstance(row[col], str) and row[col].strip() != \"\":\n",
    "                reshaped_data.append({\"cluster\": module_num, \"gene_symbol\": row[col]})\n",
    "            elif isinstance(row[col], (int, float)) and not pd.isna(row[col]):\n",
    "                # Convert numeric values to string if they appear to be gene IDs\n",
    "                reshaped_data.append(\n",
    "                    {\"cluster\": module_num, \"gene_symbol\": str(row[col])}\n",
    "                )\n",
    "# Create the reshaped dataframe\n",
    "wainberg = pd.DataFrame(reshaped_data)\n",
    "# Sort by cluster\n",
    "wainberg.sort_values(by=[\"cluster\"], inplace=True)\n",
    "# Display the Wainberg dataset\n",
    "display(wainberg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e755477",
   "metadata": {},
   "outputs": [],
   "source": [
    "schaffer = pd.read_excel(\n",
    "    os.path.join(project_root, \"data/benchmarks/U2OS Cell Map Assemblies.xlsx\")\n",
    ")\n",
    "# Select Assembly ID C5255, C5415\n",
    "schaffer = schaffer[schaffer[\"Assembly ID\"].isin([\"C5255\", \"C5415\"])].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "# Create a new dataframe to store the reshaped data\n",
    "reshaped_data = []\n",
    "\n",
    "# Process each row in the original dataframe\n",
    "for _, row in schaffer.iterrows():\n",
    "    cluster = row[\"Assembly ID\"]\n",
    "\n",
    "    # Handle the case where Proteins might be missing or NaN\n",
    "    if pd.notna(row[\"Proteins\"]):\n",
    "        # Split the space-separated proteins\n",
    "        proteins = str(row[\"Proteins\"]).split()\n",
    "\n",
    "        # Add each protein to the reshaped data\n",
    "        for protein in proteins:\n",
    "            if protein.strip():  # Skip empty strings\n",
    "                reshaped_data.append(\n",
    "                    {\"cluster\": cluster, \"gene_symbol\": protein.strip()}\n",
    "                )\n",
    "# Create the reshaped dataframe\n",
    "schaffer = pd.DataFrame(reshaped_data)\n",
    "# Rename \"Gene Symbol\" to \"gene_symbol\" and \"Assembly ID\" to \"cluster\"\n",
    "schaffer.rename(\n",
    "    columns={\n",
    "        \"Gene Symbol\": \"gene_symbol\",\n",
    "        \"Assembly ID\": \"cluster\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "# Sort by cluster\n",
    "schaffer.sort_values(by=[\"cluster\"], inplace=True)\n",
    "# Display the Schaffer dataset\n",
    "display(schaffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f383584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the uniprot_data.tsv\n",
    "uniprot_data = pd.read_csv(\n",
    "    os.path.join(project_root, \"data/benchmarks/uniprot_data.tsv\"), sep=\"\\t\"\n",
    ")\n",
    "# Create multiple rows for each gene name with position information\n",
    "expanded_rows = []\n",
    "for _, row in uniprot_data.iterrows():\n",
    "    if pd.notna(row[\"gene_names\"]):\n",
    "        gene_names = row[\"gene_names\"].split()\n",
    "        for position, gene in enumerate(gene_names):\n",
    "            new_row = row.copy()\n",
    "            new_row[\"gene_name\"] = gene\n",
    "            new_row[\"position\"] = position\n",
    "            expanded_rows.append(new_row)\n",
    "# Create expanded dataframe\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "# Sort by gene_name and position, then drop duplicates keeping the one with lower position\n",
    "expanded_df = expanded_df.sort_values([\"gene_name\", \"position\"])\n",
    "expanded_df = expanded_df.drop_duplicates(\"gene_name\", keep=\"first\")\n",
    "# Select and rename columns\n",
    "uniprot_data = expanded_df[[\"gene_name\", \"entry\", \"function\"]].rename(\n",
    "    columns={\"function\": \"uniprot_function\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge each dataset with UniProt data\n",
    "for dataset_name in [\"funk\", \"wainberg\", \"schaffer\"]:\n",
    "    dataset = locals()[dataset_name]  # Access dataset from local variables\n",
    "\n",
    "    # Merge with UniProt data\n",
    "    dataset = dataset.merge(\n",
    "        uniprot_data, how=\"left\", left_on=\"gene_symbol\", right_on=\"gene_name\"\n",
    "    ).drop(columns=\"gene_name\", errors=\"ignore\")\n",
    "\n",
    "    # Print the number of rows in the merged dataset and the number of rows missing a uniprot_function\n",
    "    print(\n",
    "        f\"{dataset_name} dataset: {len(dataset)} rows, {dataset['uniprot_function'].isna().sum()} missing uniprot_function\"\n",
    "    )\n",
    "\n",
    "    # Update the original dataset variable\n",
    "    locals()[dataset_name] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(funk)\n",
    "display(wainberg)\n",
    "display(schaffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mozzarellm import analyze_gene_clusters, reshape_to_clusters\n",
    "from mozzarellm.prompts import ROBUST_SCREEN_CONTEXT, ROBUST_CLUSTER_PROMPT\n",
    "from mozzarellm.configs import (\n",
    "    DEFAULT_OPENAI_REASONING_CONFIG,\n",
    "    DEFAULT_OPENAI_CONFIG,\n",
    "    DEFAULT_ANTHROPIC_CONFIG,\n",
    "    DEFAULT_GEMINI_CONFIG,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34006815",
   "metadata": {},
   "source": [
    "### Reshape clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472c1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_COL = \"gene_symbol\"\n",
    "CLUSTER_COL = \"cluster\"\n",
    "UNIPROT_COL = \"uniprot_function\"\n",
    "\n",
    "# Dictionary to store the processed results\n",
    "processed_data = {}\n",
    "\n",
    "for dataset_name in [\"funk\", \"wainberg\", \"schaffer\"]:\n",
    "    # Get the dataset from local variables\n",
    "    sample_data = locals()[dataset_name]\n",
    "\n",
    "    # Process with reshape_to_clusters without saving\n",
    "    cluster_df, gene_features = reshape_to_clusters(\n",
    "        input_df=sample_data,\n",
    "        gene_col=GENE_COL,\n",
    "        cluster_col=CLUSTER_COL,\n",
    "        uniprot_col=UNIPROT_COL,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Store results in dictionary for later use\n",
    "    processed_data[dataset_name] = {\n",
    "        \"clusters\": cluster_df,\n",
    "        \"gene_features\": gene_features,\n",
    "    }\n",
    "\n",
    "    # Print summary stats\n",
    "    print(\n",
    "        f\"Processed {dataset_name}: {len(cluster_df)} clusters with {len(gene_features)} unique genes\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca33a6a2",
   "metadata": {},
   "source": [
    "### Knowledge cutoffs:\n",
    "- o4-mini: May 2024\n",
    "- o3: May 2024\n",
    "- o3-mini: Sep 2023\n",
    "- gpt-4.1: May 2024\n",
    "- gpt-4o: Sep 2023\n",
    "- claude-3-7-sonnet-20250219: Nov 2024\n",
    "- claude-3-5-haiku-20241022: Apr 2024\n",
    "- gemini-2.5-pro-preview-03-25: Jan 2025\n",
    "- gemini-2.5-flash-preview-04-17: Jan 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6fdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models and configurations to test\n",
    "models_to_test = {\n",
    "    \"openai_reasoning\": {\n",
    "        \"models\": [\"o4-mini\", \"o3-mini\"],\n",
    "        \"config\": DEFAULT_OPENAI_REASONING_CONFIG,\n",
    "    },\n",
    "    \"openai\": {\"models\": [\"gpt-4.1\", \"gpt-4o\"], \"config\": DEFAULT_OPENAI_CONFIG},\n",
    "    \"anthropic\": {\n",
    "        \"models\": [\"claude-3-7-sonnet-20250219\", \"claude-3-5-haiku-20241022\"],\n",
    "        \"config\": DEFAULT_ANTHROPIC_CONFIG,\n",
    "    },\n",
    "    \"gemini\": {\n",
    "        \"models\": [\"gemini-2.5-pro-preview-03-25\", \"gemini-2.5-flash-preview-04-17\"],\n",
    "        \"config\": DEFAULT_GEMINI_CONFIG,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Dictionary to track results\n",
    "results_summary = {}\n",
    "\n",
    "# Loop through datasets\n",
    "for dataset_name in [\"funk\", \"wainberg\", \"schaffer\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    results_summary[dataset_name] = {}\n",
    "\n",
    "    # Get the processed data from our dictionary\n",
    "    cluster_df = processed_data[dataset_name][\"clusters\"]\n",
    "    gene_features = processed_data[dataset_name][\"gene_features\"]\n",
    "\n",
    "    # Loop through each configuration and model\n",
    "    for provider, provider_info in models_to_test.items():\n",
    "        models = provider_info[\"models\"]\n",
    "        config_dict = provider_info[\"config\"]\n",
    "\n",
    "        for MODEL_NAME in models:\n",
    "            print(f\"\\n{'-'*50}\")\n",
    "            print(f\"Running {MODEL_NAME} on {dataset_name}\")\n",
    "            print(f\"{'-'*50}\")\n",
    "\n",
    "            # Define results directory for this model\n",
    "            RESULTS_DIR = os.path.join(\n",
    "                project_root,\n",
    "                f\"results/{dataset_name}/{provider}/{MODEL_NAME.replace('/', '_')}\",\n",
    "            )\n",
    "            os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                # Run the analysis with in-memory DataFrames\n",
    "                results = analyze_gene_clusters(\n",
    "                    # Input data options\n",
    "                    input_df=cluster_df,\n",
    "                    # Model and configuration\n",
    "                    model_name=MODEL_NAME,\n",
    "                    config_dict=config_dict,\n",
    "                    # Analysis context and prompts\n",
    "                    screen_context=ROBUST_SCREEN_CONTEXT,\n",
    "                    cluster_analysis_prompt=ROBUST_CLUSTER_PROMPT,\n",
    "                    # Gene annotations\n",
    "                    gene_annotations_df=gene_features,\n",
    "                    # Processing options\n",
    "                    batch_size=1,\n",
    "                    # Output options\n",
    "                    output_file=f\"{RESULTS_DIR}/{MODEL_NAME.replace('/', '_')}\",\n",
    "                    save_outputs=True,\n",
    "                    outputs_to_generate=[\"json\", \"clusters\", \"flagged_genes\"],\n",
    "                )\n",
    "\n",
    "                results_summary[dataset_name][f\"{provider}_{MODEL_NAME}\"] = \"Success\"\n",
    "                print(\n",
    "                    f\"✓ Successfully analyzed {dataset_name} with {provider} {MODEL_NAME}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                results_summary[dataset_name][f\"{provider}_{MODEL_NAME}\"] = (\n",
    "                    f\"Failed: {str(e)}\"\n",
    "                )\n",
    "                print(\n",
    "                    f\"✗ Error analyzing {dataset_name} with {provider} {MODEL_NAME}: {str(e)}\"\n",
    "                )\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\\n\" + \"=\" * 80)\n",
    "print(\"BENCHMARK RESULTS SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "for dataset, model_results in results_summary.items():\n",
    "    print(f\"\\nDataset: {dataset}\")\n",
    "    for model, result in model_results.items():\n",
    "        status = \"✓\" if result == \"Success\" else \"✗\"\n",
    "        print(f\"  {status} {model}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fbefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Datasets to analyze\n",
    "datasets = [\"funk\", \"wainberg\", \"schaffer\"]\n",
    "# Models to check\n",
    "models_to_check = [\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"claude-3-5-haiku-20241022\",\n",
    "    \"gemini-2.5-flash-preview-04-17\",\n",
    "    \"gemini-2.5-pro-preview-03-25\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4o\",\n",
    "    \"o3-mini\",\n",
    "    \"o4-mini\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_gene_list(gene_str):\n",
    "    \"\"\"Parse gene list string into a list of genes, considering different separators\"\"\"\n",
    "    if not gene_str or pd.isna(gene_str):\n",
    "        return []\n",
    "\n",
    "    gene_str = str(gene_str).strip()\n",
    "\n",
    "    # Try different separators\n",
    "    for separator in [\";\", \",\", \" \"]:\n",
    "        if separator in gene_str:\n",
    "            return [g.strip() for g in gene_str.split(separator) if g.strip()]\n",
    "\n",
    "    # If no separator found but string is not empty\n",
    "    return [gene_str] if gene_str else []\n",
    "\n",
    "\n",
    "def load_data(project_root):\n",
    "    \"\"\"Load benchmark findings and prepare data structures\"\"\"\n",
    "    # Load findings.csv\n",
    "    findings_path = os.path.join(project_root, \"data/benchmarks/findings.csv\")\n",
    "    try:\n",
    "        findings_df = pd.read_csv(findings_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading findings file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # Prepare benchmark data\n",
    "    benchmark_data = defaultdict(dict)\n",
    "\n",
    "    for dataset, group in findings_df.groupby(\"dataset\"):\n",
    "        if dataset not in datasets:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nAnalyzing dataset: {dataset}\")\n",
    "\n",
    "        # Process findings for this dataset\n",
    "        for _, row in group.iterrows():\n",
    "            cluster = str(row[\"cluster\"])\n",
    "            gene = row[\"gene\"]\n",
    "            function = row[\"function\"]\n",
    "\n",
    "            if cluster not in benchmark_data[dataset]:\n",
    "                benchmark_data[dataset][cluster] = {\"function\": function, \"genes\": []}\n",
    "\n",
    "            benchmark_data[dataset][cluster][\"genes\"].append(gene)\n",
    "\n",
    "    return benchmark_data\n",
    "\n",
    "\n",
    "def load_clusters_data(project_root, dataset, model):\n",
    "    \"\"\"Load clusters data for a specific model and dataset\"\"\"\n",
    "    safe_model_name = model.replace(\"/\", \"_\")\n",
    "    clusters_file = os.path.join(\n",
    "        project_root,\n",
    "        f\"results/{dataset}/{safe_model_name}/{safe_model_name}_clusters.csv\",\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(clusters_file):\n",
    "        print(f\"No clusters file found for {model} on {dataset}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        return pd.read_csv(clusters_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading clusters for {model} on {dataset}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def analyze_model_output(project_root, benchmark_data):\n",
    "    \"\"\"Analyze and compare model outputs with benchmark data\"\"\"\n",
    "    results = defaultdict(lambda: defaultdict(dict))\n",
    "\n",
    "    for dataset, clusters in benchmark_data.items():\n",
    "        # Count benchmark clusters and genes for this dataset\n",
    "        benchmark_clusters = list(clusters.keys())\n",
    "        benchmark_genes = []\n",
    "        for cluster_data in clusters.values():\n",
    "            benchmark_genes.extend(cluster_data[\"genes\"])\n",
    "        benchmark_genes = list(set(benchmark_genes))  # Remove duplicates\n",
    "\n",
    "        for model in models_to_check:\n",
    "            print(f\"  Checking model: {model}\")\n",
    "\n",
    "            # Load clusters.csv for this model and dataset\n",
    "            safe_model_name = model.replace(\"/\", \"_\")\n",
    "            clusters_file = os.path.join(\n",
    "                project_root,\n",
    "                f\"results/{dataset}/{safe_model_name}/{safe_model_name}_clusters.csv\",\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(clusters_file):\n",
    "                # Record failure and continue to next model\n",
    "                results[dataset][model] = create_empty_result(\n",
    "                    benchmark_clusters,\n",
    "                    benchmark_genes,\n",
    "                    \"Failed - No clusters.csv found\",\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                clusters_df = pd.read_csv(clusters_file)\n",
    "            except Exception as e:\n",
    "                # Record error and continue to next model\n",
    "                results[dataset][model] = create_empty_result(\n",
    "                    benchmark_clusters,\n",
    "                    benchmark_genes,\n",
    "                    f\"Failed - Error loading file: {str(e)}\",\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Check for required columns\n",
    "            if \"cluster_id\" not in clusters_df.columns:\n",
    "                results[dataset][model] = create_empty_result(\n",
    "                    benchmark_clusters,\n",
    "                    benchmark_genes,\n",
    "                    \"Failed - Missing cluster_id column\",\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Process model output\n",
    "            results[dataset][model] = compare_model_to_benchmark(\n",
    "                clusters_df, benchmark_clusters, benchmark_genes, clusters\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def create_empty_result(benchmark_clusters, benchmark_genes, status=\"Failed\"):\n",
    "    \"\"\"Create a default result structure for failed analyses\"\"\"\n",
    "    return {\n",
    "        \"status\": status,\n",
    "        \"cluster_matches\": {},\n",
    "        \"gene_categories\": {},\n",
    "        \"total_clusters\": len(benchmark_clusters),\n",
    "        \"found_clusters\": 0,\n",
    "        \"found_cluster_percent\": 0.0,\n",
    "        \"total_genes\": len(benchmark_genes),\n",
    "        \"found_genes\": 0,\n",
    "        \"found_gene_percent\": 0.0,\n",
    "        \"novel_count\": 0,\n",
    "        \"uncharacterized_count\": 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_model_to_benchmark(\n",
    "    clusters_df, benchmark_clusters, benchmark_genes, benchmark_data\n",
    "):\n",
    "    \"\"\"Compare model output to benchmark data and compute metrics\"\"\"\n",
    "    # Find all columns that might contain genes\n",
    "    gene_columns = [\n",
    "        col\n",
    "        for col in clusters_df.columns\n",
    "        if any(term in col.lower() for term in [\"gene\", \"genes\"])\n",
    "    ]\n",
    "\n",
    "    # Convert cluster IDs to strings for comparison\n",
    "    clusters_df[\"cluster_id_str\"] = clusters_df[\"cluster_id\"].astype(str)\n",
    "\n",
    "    # Initialize tracking\n",
    "    cluster_matches = {}\n",
    "    found_clusters = []\n",
    "    gene_categories = {}\n",
    "    found_genes = []  # This will now only include properly categorized genes\n",
    "    novel_count = 0\n",
    "    uncharacterized_count = 0\n",
    "    established_count = 0\n",
    "\n",
    "    # Check each benchmark cluster\n",
    "    for benchmark_cluster, cluster_info in benchmark_data.items():\n",
    "        benchmark_function = cluster_info[\"function\"].lower()\n",
    "        benchmark_genes_for_cluster = cluster_info[\"genes\"]\n",
    "\n",
    "        # Find matching clusters in model output\n",
    "        cluster_rows = clusters_df[clusters_df[\"cluster_id_str\"] == benchmark_cluster]\n",
    "\n",
    "        if not cluster_rows.empty:\n",
    "            # Found exact cluster match\n",
    "            cluster_matches[benchmark_cluster] = \"found\"\n",
    "            found_clusters.append(benchmark_cluster)\n",
    "\n",
    "            # Check for function match\n",
    "            function_match = check_function_match(cluster_rows, benchmark_function)\n",
    "            if function_match:\n",
    "                cluster_matches[benchmark_cluster] += \"_with_matching_function\"\n",
    "\n",
    "            # Check each gene in this cluster\n",
    "            for gene in benchmark_genes_for_cluster:\n",
    "                # Get the category using the find_gene_category function\n",
    "                gene_category = find_gene_category(cluster_rows, gene, gene_columns)\n",
    "\n",
    "                # Only count genes that are properly categorized into one of the three categories\n",
    "                if gene_category in [\"established\", \"novel_role\", \"uncharacterized\"]:\n",
    "                    found_genes.append(gene)\n",
    "\n",
    "                    # Count each category\n",
    "                    if gene_category == \"novel_role\":\n",
    "                        novel_count += 1\n",
    "                    elif gene_category == \"uncharacterized\":\n",
    "                        uncharacterized_count += 1\n",
    "                    elif gene_category == \"established\":\n",
    "                        established_count += 1\n",
    "\n",
    "                # Always record the category for the gene categorization table\n",
    "                gene_categories[gene] = gene_category\n",
    "        else:\n",
    "            # No exact match, check for similar function in other clusters\n",
    "            function_cluster = find_function_in_other_clusters(\n",
    "                clusters_df, benchmark_function\n",
    "            )\n",
    "            if function_cluster:\n",
    "                cluster_matches[benchmark_cluster] = (\n",
    "                    f\"similar_function_in_{function_cluster}\"\n",
    "                )\n",
    "            else:\n",
    "                cluster_matches[benchmark_cluster] = \"not_found\"\n",
    "\n",
    "    # Calculate metrics\n",
    "    total_clusters = len(benchmark_clusters)\n",
    "    found_cluster_count = len(found_clusters)\n",
    "    found_cluster_percent = (\n",
    "        (found_cluster_count / total_clusters) * 100 if total_clusters > 0 else 0\n",
    "    )\n",
    "\n",
    "    total_genes = len(benchmark_genes)\n",
    "    found_gene_count = len(set(found_genes))  # Ensure uniqueness\n",
    "    found_gene_percent = (\n",
    "        (found_gene_count / total_genes) * 100 if total_genes > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Print summary with more detailed breakdown\n",
    "    print(\n",
    "        f\"    Found {found_cluster_count}/{total_clusters} clusters ({found_cluster_percent:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    Properly categorized genes: {found_gene_count}/{total_genes} ({found_gene_percent:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"      Established: {established_count}, Novel role: {novel_count}, Uncharacterized: {uncharacterized_count}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"status\": \"Success\",\n",
    "        \"total_clusters\": total_clusters,\n",
    "        \"found_clusters\": found_cluster_count,\n",
    "        \"found_cluster_percent\": found_cluster_percent,\n",
    "        \"cluster_matches\": cluster_matches,\n",
    "        \"total_genes\": total_genes,\n",
    "        \"found_genes\": found_gene_count,\n",
    "        \"found_gene_percent\": found_gene_percent,\n",
    "        \"established_count\": established_count,\n",
    "        \"novel_count\": novel_count,\n",
    "        \"uncharacterized_count\": uncharacterized_count,\n",
    "        \"gene_categories\": gene_categories,\n",
    "    }\n",
    "\n",
    "\n",
    "def find_gene_category(cluster_rows, gene, gene_columns):\n",
    "    \"\"\"Find gene in cluster and determine its category\"\"\"\n",
    "    # Check each row in the cluster\n",
    "    for _, row in cluster_rows.iterrows():\n",
    "        # Check each specific category column\n",
    "        if \"novel_role_genes\" in row and pd.notna(row[\"novel_role_genes\"]):\n",
    "            gene_list = parse_gene_list(row[\"novel_role_genes\"])\n",
    "            if gene in gene_list:\n",
    "                return \"novel_role\"  # Found in novel_role category\n",
    "\n",
    "        if \"uncharacterized_genes\" in row and pd.notna(row[\"uncharacterized_genes\"]):\n",
    "            gene_list = parse_gene_list(row[\"uncharacterized_genes\"])\n",
    "            if gene in gene_list:\n",
    "                return \"uncharacterized\"  # Found in uncharacterized category\n",
    "\n",
    "        if \"established_genes\" in row and pd.notna(row[\"established_genes\"]):\n",
    "            gene_list = parse_gene_list(row[\"established_genes\"])\n",
    "            if gene in gene_list:\n",
    "                return \"established\"  # Found in established category\n",
    "\n",
    "    # If we reach here, the gene wasn't found in any of the three category columns\n",
    "\n",
    "    # Check if it's in any general gene column (for the gene categorization table)\n",
    "    for _, row in cluster_rows.iterrows():\n",
    "        for gene_col in gene_columns:\n",
    "            if gene_col in row and pd.notna(row[gene_col]):\n",
    "                gene_list = parse_gene_list(row[gene_col])\n",
    "                if gene in gene_list:\n",
    "                    return \"found_but_uncategorized\"  # Found in a general gene list but not categorized\n",
    "\n",
    "    # If not found anywhere\n",
    "    return \"missing_in_cluster\"\n",
    "\n",
    "\n",
    "def generate_summary_table(results):\n",
    "    \"\"\"Generate enhanced summary table with strict gene categorization metrics\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for dataset, models in results.items():\n",
    "        for model, data in models.items():\n",
    "            # Skip failed analyses\n",
    "            if data[\"status\"] != \"Success\":\n",
    "                row = {\n",
    "                    \"Dataset\": dataset,\n",
    "                    \"Model\": model,\n",
    "                    \"Status\": data[\"status\"],\n",
    "                    \"Clusters Found\": f\"0/{data['total_clusters']} (0.0%)\",\n",
    "                    \"Genes Found\": f\"0/{data['total_genes']} (0.0%)\",\n",
    "                    \"Established Genes\": 0,\n",
    "                    \"Novel Role Genes\": 0,\n",
    "                    \"Uncharacterized Genes\": 0,\n",
    "                    \"Novel/Unchar Count\": 0,\n",
    "                    \"Novel/Unchar %\": \"0.0%\",\n",
    "                }\n",
    "                rows.append(row)\n",
    "                continue\n",
    "\n",
    "            # Calculate the sum of novel and uncharacterized genes\n",
    "            novel_unchar_count = data[\"novel_count\"] + data[\"uncharacterized_count\"]\n",
    "\n",
    "            # Calculate the percentage of novel and uncharacterized genes out of total genes\n",
    "            novel_unchar_percent = (\n",
    "                (novel_unchar_count / data[\"total_genes\"]) * 100\n",
    "                if data[\"total_genes\"] > 0\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "            # Create a row with all metrics\n",
    "            row = {\n",
    "                \"Dataset\": dataset,\n",
    "                \"Model\": model,\n",
    "                \"Status\": data[\"status\"],\n",
    "                \"Clusters Found\": f\"{data['found_clusters']}/{data['total_clusters']} ({data['found_cluster_percent']:.1f}%)\",\n",
    "                \"Genes Found\": f\"{data['found_genes']}/{data['total_genes']} ({data['found_gene_percent']:.1f}%)\",\n",
    "                \"Established Genes\": data.get(\n",
    "                    \"established_count\", 0\n",
    "                ),  # Add established count\n",
    "                \"Novel Role Genes\": data[\"novel_count\"],\n",
    "                \"Uncharacterized Genes\": data[\"uncharacterized_count\"],\n",
    "                \"Novel/Unchar Count\": novel_unchar_count,\n",
    "                \"Novel/Unchar %\": f\"{novel_unchar_percent:.1f}%\",\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if rows:\n",
    "        return pd.DataFrame(rows)\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_cluster_table(results, project_root):\n",
    "    \"\"\"Generate a cluster information table with biological process and follow-up suggestion\"\"\"\n",
    "    # Collect all unique datasets and clusters\n",
    "    dataset_clusters = {}\n",
    "    all_models = set()\n",
    "\n",
    "    for dataset, models in results.items():\n",
    "        dataset_clusters[dataset] = set()\n",
    "        for model, data in models.items():\n",
    "            all_models.add(model)\n",
    "            if data[\"status\"] == \"Success\":\n",
    "                dataset_clusters[dataset].update(data[\"cluster_matches\"].keys())\n",
    "\n",
    "    # Sort models for consistent column order\n",
    "    sorted_models = sorted(all_models)\n",
    "\n",
    "    # Create rows for the table\n",
    "    rows = []\n",
    "    for dataset, clusters in dataset_clusters.items():\n",
    "        for cluster in sorted(clusters):\n",
    "            # Create a row for each dataset-cluster combination\n",
    "            row = {\"Dataset\": dataset, \"Cluster\": cluster}\n",
    "\n",
    "            # Add columns for each model\n",
    "            for model in sorted_models:\n",
    "                if (\n",
    "                    model in results[dataset]\n",
    "                    and results[dataset][model][\"status\"] == \"Success\"\n",
    "                ):\n",
    "                    # Get the biological process and follow-up suggestion\n",
    "                    process, followup = get_cluster_info(\n",
    "                        project_root, dataset, model, cluster\n",
    "                    )\n",
    "                    row[f\"{model}_biological_process\"] = process\n",
    "                    row[f\"{model}_follow_up\"] = followup\n",
    "                else:\n",
    "                    row[f\"{model}_biological_process\"] = \"N/A\"\n",
    "                    row[f\"{model}_follow_up\"] = \"N/A\"\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if rows:\n",
    "        return pd.DataFrame(rows)\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_cluster_info(project_root, dataset, model, cluster):\n",
    "    \"\"\"Get both biological process and follow-up suggestion for a specific cluster\"\"\"\n",
    "    # Load the clusters file for this model using the provided function\n",
    "    clusters_df = load_clusters_data(project_root, dataset, model)\n",
    "\n",
    "    if clusters_df is None:\n",
    "        return \"File not found\", \"File not found\"\n",
    "\n",
    "    # Convert cluster IDs to strings for comparison\n",
    "    clusters_df[\"cluster_id_str\"] = clusters_df[\"cluster_id\"].astype(str)\n",
    "\n",
    "    # Find the matching cluster\n",
    "    cluster_rows = clusters_df[clusters_df[\"cluster_id_str\"] == cluster]\n",
    "\n",
    "    if cluster_rows.empty:\n",
    "        return \"Cluster not found\", \"Cluster not found\"\n",
    "\n",
    "    # Get biological process\n",
    "    process = \"No process found\"\n",
    "    for column in [\n",
    "        \"cluster_biological_process\",\n",
    "        \"biological_process\",\n",
    "        \"function\",\n",
    "        \"description\",\n",
    "    ]:\n",
    "        if column in cluster_rows.columns and pd.notna(cluster_rows.iloc[0][column]):\n",
    "            process = cluster_rows.iloc[0][column]\n",
    "            break\n",
    "\n",
    "    # Get follow-up suggestion\n",
    "    followup = \"No follow-up found\"\n",
    "    if \"follow_up_suggestion\" in cluster_rows.columns and pd.notna(\n",
    "        cluster_rows.iloc[0][\"follow_up_suggestion\"]\n",
    "    ):\n",
    "        followup = cluster_rows.iloc[0][\"follow_up_suggestion\"]\n",
    "\n",
    "    return process, followup\n",
    "\n",
    "\n",
    "def generate_streamlined_gene_table(results):\n",
    "    \"\"\"Generate a streamlined gene categorization table as requested\"\"\"\n",
    "    # Collect all unique datasets and genes\n",
    "    dataset_genes = {}\n",
    "    all_models = set()\n",
    "\n",
    "    for dataset, models in results.items():\n",
    "        dataset_genes[dataset] = set()\n",
    "        for model, data in models.items():\n",
    "            all_models.add(model)\n",
    "            if data[\"status\"] == \"Success\":\n",
    "                dataset_genes[dataset].update(data[\"gene_categories\"].keys())\n",
    "\n",
    "    # Sort models for consistent column order\n",
    "    sorted_models = sorted(all_models)\n",
    "\n",
    "    # Create rows for the table\n",
    "    rows = []\n",
    "    for dataset, genes in dataset_genes.items():\n",
    "        for gene in sorted(genes):\n",
    "            # Create a row for each dataset-gene combination\n",
    "            row = {\"Dataset\": dataset, \"Gene\": gene}\n",
    "\n",
    "            # Add a column for each model\n",
    "            for model in sorted_models:\n",
    "                if (\n",
    "                    model in results[dataset]\n",
    "                    and results[dataset][model][\"status\"] == \"Success\"\n",
    "                ):\n",
    "                    gene_categories = results[dataset][model][\"gene_categories\"]\n",
    "                    # Get this gene's category for this model (or 'missing_cluster' if not found)\n",
    "                    category = gene_categories.get(gene, \"missing_from_all_clusters\")\n",
    "                    row[model] = category\n",
    "                else:\n",
    "                    row[model] = \"N/A\"  # Model analysis failed\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if rows:\n",
    "        return pd.DataFrame(rows)\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_reports(project_root, results):\n",
    "    \"\"\"Generate all summary reports\"\"\"\n",
    "    # Basic summary table\n",
    "    summary_df = generate_summary_table(results)\n",
    "    if summary_df is not None:\n",
    "        print(\"\\n=== SUMMARY TABLE ===\")\n",
    "        print(summary_df.to_string())\n",
    "\n",
    "    # Generate streamlined gene categorization table\n",
    "    gene_table = generate_streamlined_gene_table(results)\n",
    "    if gene_table is not None:\n",
    "        print(\"\\n=== GENE CATEGORIZATION TABLE ===\")\n",
    "        print(gene_table.to_string())\n",
    "\n",
    "    # Generate cluster information table\n",
    "    cluster_table = generate_cluster_table(results, project_root)\n",
    "    if cluster_table is not None:\n",
    "        print(\"\\n=== CLUSTER INFORMATION TABLE ===\")\n",
    "        print(cluster_table.to_string())\n",
    "\n",
    "    return summary_df, gene_table, cluster_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark data\n",
    "benchmark_data = load_data(project_root)\n",
    "\n",
    "# Analyze model outputs\n",
    "results = analyze_model_output(project_root, benchmark_data)\n",
    "\n",
    "# Generate and save reports\n",
    "summary_df, gene_table, cluster_table = generate_reports(project_root, results)\n",
    "\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d62c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any quotes in the tables\n",
    "def clean_table(df):\n",
    "    \"\"\"Remove quotes from all string columns in the DataFrame\"\"\"\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        df[col] = df[col].str.replace('\"', \"\", regex=False)\n",
    "        df[col] = df[col].str.replace(\"'\", \"\", regex=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Clean the tables\n",
    "summary_df = clean_table(summary_df)\n",
    "gene_table = clean_table(gene_table)\n",
    "cluster_table = clean_table(cluster_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3dde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the tables to excel sheets\n",
    "output_file = os.path.join(project_root, \"data/benchmarks/analysis_results.xlsx\")\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    gene_table.to_excel(writer, sheet_name=\"Gene Categorization\", index=False)\n",
    "    cluster_table.to_excel(writer, sheet_name=\"Cluster Information\", index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Extract percentages from strings\n",
    "summary_df[\"Novel/Unchar %\"] = (\n",
    "    summary_df[\"Novel/Unchar %\"].str.rstrip(\"%\").astype(float)\n",
    ")\n",
    "\n",
    "# Extract Genes Found percentage from the string format \"X/Y (Z%)\"\n",
    "summary_df[\"Genes Found %\"] = summary_df[\"Genes Found\"].apply(\n",
    "    lambda x: float(x.split(\"(\")[1].split(\"%\")[0])\n",
    ")\n",
    "\n",
    "# Create a model type mapping\n",
    "model_types = {\n",
    "    \"claude-3-7-sonnet-20250219\": \"Claude\",\n",
    "    \"claude-3-5-haiku-20241022\": \"Claude\",\n",
    "    \"gemini-2.5-flash-preview-04-17\": \"Gemini\",\n",
    "    \"gemini-2.5-pro-preview-03-25\": \"Gemini\",\n",
    "    \"gpt-4.1\": \"GPT\",\n",
    "    \"gpt-4o\": \"GPT\",\n",
    "    \"o3-mini\": \"GPT\",\n",
    "    \"o4-mini\": \"GPT\",\n",
    "}\n",
    "\n",
    "# Add model type column\n",
    "summary_df[\"Model Type\"] = summary_df[\"Model\"].map(model_types)\n",
    "\n",
    "# Define base colors for each model type\n",
    "type_base_colors = {\n",
    "    \"Claude\": \"#9467bd\",  # Purple\n",
    "    \"Gemini\": \"#2ca02c\",  # Green\n",
    "    \"GPT\": \"#d62728\",  # Red\n",
    "}\n",
    "\n",
    "# Group models by type\n",
    "models_by_type = {}\n",
    "for model, model_type in model_types.items():\n",
    "    if model_type not in models_by_type:\n",
    "        models_by_type[model_type] = []\n",
    "    models_by_type[model_type].append(model)\n",
    "\n",
    "# Create different shades for each model within the same family\n",
    "model_colors = {}\n",
    "for model_type, models in models_by_type.items():\n",
    "    base_color = mcolors.to_rgb(type_base_colors[model_type])\n",
    "\n",
    "    # Create shades based on number of models\n",
    "    if len(models) == 2:\n",
    "        # For 2 models: one darker, one lighter\n",
    "        model_colors[models[0]] = mcolors.to_hex(\n",
    "            [c * 0.7 for c in base_color]\n",
    "        )  # Darker\n",
    "        model_colors[models[1]] = mcolors.to_hex(\n",
    "            [min(1.0, c * 1.3) for c in base_color]\n",
    "        )  # Lighter\n",
    "    elif len(models) == 4:  # For GPT with 4 models\n",
    "        # Create 4 shades from dark to light\n",
    "        shadings = [0.6, 0.8, 1.0, 1.2]\n",
    "        for i, model in enumerate(models):\n",
    "            shade_factor = shadings[i]\n",
    "            model_colors[model] = mcolors.to_hex(\n",
    "                [min(1.0, c * shade_factor) for c in base_color]\n",
    "            )\n",
    "    else:\n",
    "        # If just one model, use the base color\n",
    "        for model in models:\n",
    "            model_colors[model] = type_base_colors[model_type]\n",
    "\n",
    "# Set the style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure with two panels\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 12), sharex=True)\n",
    "\n",
    "# Panel 1: Novel/Uncharacterized Genes\n",
    "sns.barplot(\n",
    "    data=summary_df,\n",
    "    x=\"Dataset\",\n",
    "    y=\"Novel/Unchar %\",\n",
    "    hue=\"Model\",\n",
    "    palette=model_colors,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "# Set the title and labels for panel 1\n",
    "ax1.set_title(\"Novel/Uncharacterized Gene Discovery by Model and Dataset\", fontsize=16)\n",
    "ax1.set_ylabel(\"Novel/Uncharacterized Genes (%)\", fontsize=14)\n",
    "ax1.set_ylim(0, 100)  # Slightly higher to give space at the top\n",
    "ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x)}%\"))\n",
    "ax1.yaxis.set_major_locator(MaxNLocator(nbins=10, integer=True))\n",
    "ax1.get_legend().remove()\n",
    "\n",
    "# Panel 2: Genes Found\n",
    "sns.barplot(\n",
    "    data=summary_df,\n",
    "    x=\"Dataset\",\n",
    "    y=\"Genes Found %\",\n",
    "    hue=\"Model\",\n",
    "    palette=model_colors,\n",
    "    ax=ax2,\n",
    ")\n",
    "\n",
    "# Set the title and labels for panel 2\n",
    "ax2.set_title(\"Genes Found by Model and Dataset\", fontsize=16)\n",
    "ax2.set_xlabel(\"Dataset\", fontsize=14)\n",
    "ax2.set_ylabel(\"Genes Found (%)\", fontsize=14)\n",
    "ax2.set_ylim(0, 100)  # Slightly higher to give space at the top\n",
    "ax2.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{int(x)}%\"))\n",
    "ax2.yaxis.set_major_locator(MaxNLocator(nbins=10, integer=True))\n",
    "ax2.get_legend().remove()\n",
    "\n",
    "# Create custom legend\n",
    "legend_elements = []\n",
    "\n",
    "# Add model elements - group by model type for organization\n",
    "for model_type in type_base_colors.keys():\n",
    "    # Add model type header\n",
    "    legend_elements.append(\n",
    "        Line2D([0], [0], color=\"white\", lw=0, label=f\"{model_type} Models:\")\n",
    "    )\n",
    "\n",
    "    # Add models of this type\n",
    "    for model in sorted(models_by_type[model_type]):\n",
    "        color = model_colors[model]\n",
    "        # Create a simplified model name for display\n",
    "        if \"claude\" in model:\n",
    "            display_name = (\n",
    "                model.replace(\"claude-\", \"Claude \")\n",
    "                .replace(\"-20250219\", \"\")\n",
    "                .replace(\"-20241022\", \"\")\n",
    "            )\n",
    "        elif \"gemini\" in model:\n",
    "            display_name = (\n",
    "                model.replace(\"gemini-\", \"Gemini \")\n",
    "                .replace(\"-preview-04-17\", \"\")\n",
    "                .replace(\"-preview-03-25\", \"\")\n",
    "            )\n",
    "        elif \"gpt\" in model:\n",
    "            display_name = model.replace(\"gpt-\", \"GPT \")\n",
    "        else:\n",
    "            display_name = model\n",
    "\n",
    "        legend_elements.append(Line2D([0], [0], color=color, lw=4, label=display_name))\n",
    "\n",
    "    # Add a separator after each group\n",
    "    if model_type != list(type_base_colors.keys())[-1]:\n",
    "        legend_elements.append(Line2D([0], [0], color=\"white\", lw=0, label=\"\"))\n",
    "\n",
    "# Add the legend outside the plot area, positioned between the two panels\n",
    "fig.legend(\n",
    "    handles=legend_elements, loc=\"center right\", bbox_to_anchor=(0.98, 0.5), fontsize=10\n",
    ")\n",
    "\n",
    "# Adjust layout for the legend and spacing between panels\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.8, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34a516d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mozzarellm_editable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
